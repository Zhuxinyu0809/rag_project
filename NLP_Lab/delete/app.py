import streamlit as st
import json
from datetime import datetime
from typing import List, Dict, Any

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="RAG Question Answering System",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .document-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border-left: 4px solid #1f77b4;
    }
    .answer-box {
        background-color: #e8f4f8;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border-left: 4px solid #28a745;
        margin: 1rem 0;
    }
    .reasoning-step {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border-left: 4px solid #ffc107;
    }
    .metric-card {
        background-color: white;
        padding: 1rem;
        border-radius: 0.5rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'current_query' not in st.session_state:
    st.session_state.current_query = ""
if 'show_reasoning' not in st.session_state:
    st.session_state.show_reasoning = False

# ============= è¾…åŠ©å‡½æ•° =============

def display_retrieved_documents(docs: List[Dict[str, Any]]):
    """æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£"""
    st.subheader("ğŸ“š Retrieved Documents")
    
    for idx, doc in enumerate(docs, 1):
        with st.expander(f"Document {idx} - Score: {doc.get('score', 0):.4f}"):
            st.markdown(f"**Document ID:** `{doc.get('id', 'N/A')}`")
            st.markdown(f"**Content:**")
            st.write(doc.get('text', 'No content available'))

def display_reasoning_steps(steps: List[Dict[str, Any]]):
    """æ˜¾ç¤ºæ¨ç†æ­¥éª¤ï¼ˆæ™ºèƒ½ä½“å·¥ä½œæµï¼‰"""
    st.subheader("ğŸ§  Reasoning Process")
    
    for idx, step in enumerate(steps, 1):
        st.markdown(f"""
        <div class="reasoning-step">
            <strong>Step {idx}: {step.get('type', 'Unknown')}</strong><br>
            {step.get('content', 'No content')}
        </div>
        """, unsafe_allow_html=True)

def display_answer(answer: str):
    """æ˜¾ç¤ºç­”æ¡ˆ"""
    st.markdown(f"""
    <div class="answer-box">
        <h3 style="color: #28a745; margin-top: 0;">âœ… Answer</h3>
        <p style="font-size: 1.1rem; margin-bottom: 0;">{answer}</p>
    </div>
    """, unsafe_allow_html=True)

def display_chat_history():
    """æ˜¾ç¤ºå¯¹è¯å†å²"""
    for entry in st.session_state.chat_history:
        with st.chat_message("user"):
            st.write(entry['question'])
        with st.chat_message("assistant"):
            st.write(entry['answer'])

# ============= ä¸»ç•Œé¢ =============

# æ ‡é¢˜
st.markdown('<h1 class="main-header">ğŸ¤– RAG Question Answering System</h1>', unsafe_allow_html=True)
st.markdown("---")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ System Configuration")
    
    # æ£€ç´¢æ–¹æ³•é€‰æ‹©
    st.subheader("Retrieval Method")
    retrieval_method = st.selectbox(
        "Select retrieval method:",
        ["BM25", "Dense (E5)", "Dense (BGE)", "ColBERT", "Hybrid"],
        help="Choose the retrieval algorithm"
    )
    
    # ç”Ÿæˆæ¨¡å‹é€‰æ‹©
    st.subheader("Generation Model")
    generation_model = st.selectbox(
        "Select model:",
        ["Qwen2.5-0.5B-Instruct", "Qwen2.5-1.5B-Instruct", 
         "Qwen2.5-3B-Instruct", "Qwen2.5-7B-Instruct"],
        index=2
    )
    
    # é«˜çº§åŠŸèƒ½
    st.subheader("Advanced Features")
    enable_multiturn = st.checkbox("Enable Multi-turn Dialogue", value=False)
    enable_agentic = st.checkbox("Enable Agentic Workflow", value=False)
    show_reasoning = st.checkbox("Show Reasoning Steps", value=False)
    
    # æ£€ç´¢å‚æ•°
    st.subheader("Retrieval Parameters")
    top_k = st.slider("Top K documents:", 1, 20, 10)
    
    # ç”Ÿæˆå‚æ•°
    st.subheader("Generation Parameters")
    temperature = st.slider("Temperature:", 0.0, 1.0, 0.7, 0.1)
    max_tokens = st.slider("Max tokens:", 128, 1024, 512, 64)
    
    st.markdown("---")
    
    # ç»Ÿè®¡ä¿¡æ¯
    st.subheader("ğŸ“Š Statistics")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Queries", len(st.session_state.chat_history))
    with col2:
        st.metric("Model", generation_model.split('-')[1])
    
    # æ¸…é™¤å†å²æŒ‰é’®
    if st.button("ğŸ—‘ï¸ Clear History", use_container_width=True):
        st.session_state.chat_history = []
        st.rerun()

# ä¸»å†…å®¹åŒºåŸŸ
tab1, tab2, tab3 = st.tabs(["ğŸ’¬ Query", "ğŸ“œ History", "â„¹ï¸ About"])

with tab1:
    # æŸ¥è¯¢è¾“å…¥åŒºåŸŸ
    st.subheader("Enter Your Question")
    
    query_input = st.text_area(
        "Question:",
        placeholder="Enter your question here...",
        height=100,
        key="query_input"
    )
    
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        submit_button = st.button("ğŸ” Search & Answer", type="primary", use_container_width=True)
    with col2:
        if enable_multiturn:
            st.info("Multi-turn: ON")
        else:
            st.text("Multi-turn: OFF")
    with col3:
        if enable_agentic:
            st.info("Agentic: ON")
        else:
            st.text("Agentic: OFF")
    
    # å¤„ç†æŸ¥è¯¢
    if submit_button and query_input:
        with st.spinner("ğŸ”„ Processing your query..."):
            # ======= è¿™é‡Œæ¥å…¥ä½ çš„RAGç³»ç»Ÿ =======
            # ç¤ºä¾‹æ•°æ® - æ›¿æ¢ä¸ºå®é™…çš„ç³»ç»Ÿè°ƒç”¨
            
            # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
            retrieved_docs = [
                {
                    "id": "doc_001",
                    "text": "Barack Obama was born in Honolulu, Hawaii on August 4, 1961. He served as the 44th president of the United States from 2009 to 2017.",
                    "score": 0.9523
                },
                {
                    "id": "doc_045",
                    "text": "Michelle Obama was born Michelle LaVaughn Robinson on January 17, 1964, in Chicago, Illinois. She became the First Lady of the United States in 2009.",
                    "score": 0.8834
                },
                {
                    "id": "doc_102",
                    "text": "Hawaii is a U.S. state located in the Pacific Ocean. It became the 50th state in 1959.",
                    "score": 0.7291
                }
            ]
            
            # æ¨¡æ‹Ÿæ¨ç†æ­¥éª¤ï¼ˆå¦‚æœå¯ç”¨äº†æ™ºèƒ½ä½“å·¥ä½œæµï¼‰
            reasoning_steps = []
            if enable_agentic:
                reasoning_steps = [
                    {
                        "type": "Query Analysis",
                        "content": f"Analyzing query: '{query_input}'. Identified as a factual question requiring biographical information."
                    },
                    {
                        "type": "Query Decomposition",
                        "content": "Decomposed into sub-queries: 1) Who is Barack Obama? 2) Where was he born?"
                    },
                    {
                        "type": "Retrieval",
                        "content": f"Retrieved {len(retrieved_docs)} relevant documents using {retrieval_method} method."
                    },
                    {
                        "type": "Answer Synthesis",
                        "content": "Synthesizing answer from retrieved evidence..."
                    },
                    {
                        "type": "Self-Check",
                        "content": "Verified answer against retrieved documents. Confidence: High."
                    }
                ]
            
            # æ¨¡æ‹Ÿç”Ÿæˆçš„ç­”æ¡ˆ
            answer = "Barack Obama was born in Honolulu, Hawaii, on August 4, 1961."
            
            # ======= å®é™…ç³»ç»Ÿè°ƒç”¨ç¤ºä¾‹ =======
            # from your_rag_system import RAGSystem
            # rag_system = RAGSystem(
            #     retrieval_method=retrieval_method,
            #     generation_model=generation_model,
            #     top_k=top_k,
            #     temperature=temperature,
            #     max_tokens=max_tokens
            # )
            # result = rag_system.query(
            #     query_input,
            #     enable_multiturn=enable_multiturn,
            #     enable_agentic=enable_agentic
            # )
            # retrieved_docs = result['retrieved_docs']
            # answer = result['answer']
            # reasoning_steps = result.get('reasoning_steps', [])
            
        # æ˜¾ç¤ºç»“æœ
        st.success("âœ¨ Query processed successfully!")
        
        # æ˜¾ç¤ºç­”æ¡ˆ
        display_answer(answer)
        
        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£
        display_retrieved_documents(retrieved_docs[:top_k])
        
        # æ˜¾ç¤ºæ¨ç†æ­¥éª¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if enable_agentic and show_reasoning and reasoning_steps:
            display_reasoning_steps(reasoning_steps)
        
        # ä¿å­˜åˆ°å†å²è®°å½•
        st.session_state.chat_history.append({
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "question": query_input,
            "answer": answer,
            "retrieved_docs": retrieved_docs,
            "reasoning_steps": reasoning_steps if enable_agentic else [],
            "config": {
                "retrieval_method": retrieval_method,
                "model": generation_model,
                "top_k": top_k
            }
        })
        
        # å¯¼å‡ºé€‰é¡¹
        st.markdown("---")
        col1, col2 = st.columns(2)
        with col1:
            # å¯¼å‡ºå½“å‰ç»“æœ
            result_json = json.dumps({
                "id": f"query_{len(st.session_state.chat_history)}",
                "question": query_input,
                "answer": answer,
                "retrieved_docs": [[doc['id'], doc['score']] for doc in retrieved_docs[:top_k]]
            }, indent=2)
            
            st.download_button(
                label="ğŸ“¥ Download Result (JSON)",
                data=result_json,
                file_name=f"result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
        
        with col2:
            # å¤åˆ¶ç­”æ¡ˆåˆ°å‰ªè´´æ¿
            st.code(answer, language=None)

with tab2:
    st.subheader("ğŸ“œ Query History")
    
    if st.session_state.chat_history:
        for idx, entry in enumerate(reversed(st.session_state.chat_history), 1):
            with st.expander(f"Query {len(st.session_state.chat_history) - idx + 1} - {entry['timestamp']}"):
                st.markdown(f"**Question:** {entry['question']}")
                st.markdown(f"**Answer:** {entry['answer']}")
                st.markdown(f"**Configuration:** {entry['config']['retrieval_method']} + {entry['config']['model']}")
                
                if st.button(f"View Details", key=f"details_{idx}"):
                    st.json(entry)
    else:
        st.info("No query history yet. Start by asking a question in the Query tab!")
    
    # æ‰¹é‡å¯¼å‡º
    if st.session_state.chat_history:
        st.markdown("---")
        all_results = []
        for idx, entry in enumerate(st.session_state.chat_history, 1):
            all_results.append({
                "id": f"query_{idx}",
                "question": entry['question'],
                "answer": entry['answer'],
                "retrieved_docs": [[doc['id'], doc['score']] for doc in entry['retrieved_docs'][:10]]
            })
        
        all_results_json = "\n".join([json.dumps(r) for r in all_results])
        
        st.download_button(
            label="ğŸ“¥ Export All Results (JSONL)",
            data=all_results_json,
            file_name=f"test_prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl",
            mime="application/json"
        )

with tab3:
    st.subheader("â„¹ï¸ About This System")
    
    st.markdown("""
    ### COMP5423 RAG Question Answering System
    
    This system implements a **Retrieval-Augmented Generation (RAG)** pipeline for multi-hop question answering
    based on the HotpotQA dataset.
    
    #### Features:
    - ğŸ” **Multiple Retrieval Methods**: BM25, Dense Retrieval, ColBERT, Hybrid
    - ğŸ¤– **Flexible Generation**: Qwen2.5 models (0.5B to 7B)
    - ğŸ’¬ **Multi-turn Dialogue**: Context-aware conversation
    - ğŸ§  **Agentic Workflow**: Query decomposition, self-checking, reasoning
    - ğŸ“Š **Comprehensive Evaluation**: EM and nDCG@10 metrics
    
    #### System Architecture:
    ```
    User Query â†’ Retrieval Module â†’ Generation Module â†’ Answer
                      â†“                    â†“
                 Vector Store        LLM (Qwen2.5)
    ```
    
    #### Usage:
    1. Configure retrieval and generation settings in the sidebar
    2. Enter your question in the Query tab
    3. View retrieved documents and generated answer
    4. Export results for evaluation
    
    #### Developed by:
    Group X - COMP5423 Natural Language Processing
    
    ğŸ“§ Contact: {runyang.you, xin404.zhang}@connect.polyu.hk
    """)
    
    # ç³»ç»Ÿä¿¡æ¯
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Version", "1.0.0")
    with col2:
        st.metric("Dataset", "HQ-small")
    with col3:
        st.metric("Documents", "144,718")

# é¡µè„š
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: #666;'>"
    "COMP5423 Group Project - RAG System | PolyU 2025"
    "</div>",
    unsafe_allow_html=True
)