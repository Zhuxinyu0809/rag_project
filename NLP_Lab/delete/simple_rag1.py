"""
æ”¯æŒåœ¨UIç•Œé¢é€‰æ‹©Qwenæ¨¡å‹çš„RAGç³»ç»Ÿ

åŠŸèƒ½ï¼š
1. åœ¨ç•Œé¢ä¸‹æ‹‰èœå•é€‰æ‹©ä¸åŒçš„Qwenæ¨¡å‹
2. å®æ—¶åˆ‡æ¢æ¨¡å‹
3. æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹
"""

import json
from typing import List, Dict, Tuple, Optional
from datasets import load_dataset
import bm25s
import Stemmer
import gradio as gr
from openai import OpenAI

# ============================================================================
# é…ç½®
# ============================================================================
API_KEY = "sk-uiunawvmaprjvukpszbuponajpulqjvjwxwemgftljnqbdzc"  # âš ï¸ å¡«å…¥ä½ çš„å¯†é’¥
API_BASE_URL = "https://api.siliconflow.cn/v1"

# å®šä¹‰æ‰€æœ‰å¯èƒ½çš„Qwenæ¨¡å‹ï¼ˆæŒ‰æ€§èƒ½æ’åºï¼‰
AVAILABLE_MODELS = {
    # æ˜¾ç¤ºåç§°: å®é™…çš„APIæ¨¡å‹å
    "Qwen2.5-7B (æ¨è)": [
        "Qwen/Qwen2.5-7B-Instruct",
        "Qwen2.5-7B-Instruct",
        "qwen2.5-7b-instruct"
    ],
    "Qwen2-7B ": [
        "Qwen/Qwen2-7B-Instruct",
        "Qwen2-7B-Instruct",
        "qwen2-7b-instruct"
    ]
}


# ============================================================================
# æ•°æ®åŠ è½½
# ============================================================================
class DataLoader:
    def __init__(self):
        print("ğŸ“š æ­£åœ¨åŠ è½½æ•°æ®é›†...")
        self.dataset = load_dataset("izhx/COMP5423-25Fall-HQ-small")
        self.collection = self.dataset['collection']
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼æ–‡æ¡£åº“: {len(self.collection)} æ¡")
    
    def get_documents(self) -> List[Dict]:
        return [{'id': item['id'], 'text': item['text']} for item in self.collection]


# ============================================================================
# BM25æ£€ç´¢å™¨
# ============================================================================
class BM25Retriever:
    def __init__(self, documents: List[Dict]):
        print("\nğŸ” æ­£åœ¨åˆå§‹åŒ–BM25æ£€ç´¢å™¨...")
        self.documents = documents
        self.doc_ids = [doc['id'] for doc in documents]
        
        corpus_texts = [doc['text'] for doc in documents]
        stemmer = Stemmer.Stemmer("english")
        
        print("   æ­£åœ¨å»ºç«‹ç´¢å¼•...")
        corpus_tokens = bm25s.tokenize(corpus_texts, stopwords="en", stemmer=stemmer)
        self.retriever = bm25s.BM25()
        self.retriever.index(corpus_tokens)
        self.stemmer = stemmer
        print(f"âœ… BM25ç´¢å¼•åˆ›å»ºå®Œæˆï¼")
    
    def search(self, query: str, top_k: int = 10) -> List[Dict]:
        query_tokens = bm25s.tokenize(query, stopwords="en", stemmer=self.stemmer)
        results, scores = self.retriever.retrieve(query_tokens, k=top_k)
        return [
            {'id': self.doc_ids[idx], 'text': self.documents[idx]['text'], 'score': float(score)}
            for idx, score in zip(results[0], scores[0])
        ]


# ============================================================================
# å¯åˆ‡æ¢æ¨¡å‹çš„ç”Ÿæˆå™¨
# ============================================================================
class FlexibleAPIGenerator:
    """æ”¯æŒåŠ¨æ€åˆ‡æ¢æ¨¡å‹çš„ç”Ÿæˆå™¨"""
    
    def __init__(self):
        print(f"\nğŸ¤– æ­£åœ¨åˆå§‹åŒ–APIå®¢æˆ·ç«¯...")
        
        # æ£€æŸ¥APIå¯†é’¥
        if API_KEY.startswith("sk-ä½ çš„"):
            print("\n" + "="*70)
            print("âŒ é”™è¯¯ï¼šè¯·å…ˆè®¾ç½®APIå¯†é’¥ï¼")
            print("="*70)
            raise ValueError("æœªè®¾ç½®APIå¯†é’¥")
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        self.client = OpenAI(api_key=API_KEY, base_url=API_BASE_URL)
        
        # æµ‹è¯•å¹¶ç¼“å­˜æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
        self.available_models = self._test_all_models()
        
        # è®¾ç½®é»˜è®¤æ¨¡å‹
        if self.available_models:
            self.current_model = list(self.available_models.values())[0]
            print(f"âœ… é»˜è®¤æ¨¡å‹: {self.current_model}")
        else:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
    
    def _test_all_models(self) -> Dict[str, str]:
        """æµ‹è¯•æ‰€æœ‰æ¨¡å‹ï¼Œè¿”å›å¯ç”¨çš„æ¨¡å‹æ˜ å°„"""
        print("\nğŸ” æ­£åœ¨æµ‹è¯•æ‰€æœ‰æ¨¡å‹çš„å¯ç”¨æ€§...")
        available = {}
        
        for display_name, possible_names in AVAILABLE_MODELS.items():
            print(f"\næµ‹è¯• {display_name}:")
            
            for model_name in possible_names:
                try:
                    print(f"   å°è¯•: {model_name}")
                    
                    # æµ‹è¯•è¯·æ±‚
                    response = self.client.chat.completions.create(
                        model=model_name,
                        messages=[{"role": "user", "content": "Hi"}],
                        max_tokens=5,
                        timeout=10
                    )
                    
                    # æˆåŠŸï¼è®°å½•è¿™ä¸ªæ¨¡å‹
                    available[display_name] = model_name
                    print(f"   âœ… å¯ç”¨ï¼ä½¿ç”¨: {model_name}")
                    break  # æ‰¾åˆ°ä¸€ä¸ªå°±å¤Ÿäº†
                    
                except Exception as e:
                    error_str = str(e)
                    if "does not exist" in error_str:
                        print(f"   âŒ ä¸å­˜åœ¨")
                    else:
                        print(f"   âŒ é”™è¯¯: {error_str[:50]}")
                    continue
            
            if display_name not in available:
                print(f"   âš ï¸  {display_name} ä¸å¯ç”¨")
        
        print(f"\nâœ… æ‰¾åˆ° {len(available)} ä¸ªå¯ç”¨æ¨¡å‹")
        return available
    
    def get_available_model_names(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹çš„æ˜¾ç¤ºåç§°åˆ—è¡¨ï¼ˆä¾›UIä½¿ç”¨ï¼‰"""
        return list(self.available_models.keys())
    
    def switch_model(self, display_name: str) -> str:
        """åˆ‡æ¢åˆ°æŒ‡å®šçš„æ¨¡å‹"""
        if display_name in self.available_models:
            self.current_model = self.available_models[display_name]
            print(f"ğŸ”„ åˆ‡æ¢åˆ°æ¨¡å‹: {self.current_model}")
            return self.current_model
        else:
            print(f"âš ï¸  æ¨¡å‹ {display_name} ä¸å¯ç”¨")
            return self.current_model
    
    def create_prompt(self, query: str, documents: List[Dict]) -> str:
        """åˆ›å»ºæç¤ºè¯"""
        context = ""
        for i, doc in enumerate(documents[:5], 1):
            text = doc['text'][:400]
            context += f"[Document {i}]\n{text}\n\n"
        
        prompt = f"""Answer the question based on the provided documents.

Documents:
{context}

Question: {query}

Provide a concise answer. If the documents don't contain enough information, say "Cannot answer based on provided documents."

Answer:"""
        return prompt
    
    def generate(self, query: str, documents: List[Dict]) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        prompt = self.create_prompt(query, documents)
        
        try:
            print(f"   ğŸ¤– ä½¿ç”¨æ¨¡å‹ {self.current_model} ç”Ÿæˆç­”æ¡ˆ...")
            
            response = self.client.chat.completions.create(
                model=self.current_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.7
            )
            
            answer = response.choices[0].message.content.strip()
            print(f"   âœ… ç”ŸæˆæˆåŠŸ")
            return answer
            
        except Exception as e:
            error_msg = str(e)
            print(f"   âŒ ç”Ÿæˆå¤±è´¥: {error_msg[:100]}")
            return f"ç”Ÿæˆå¤±è´¥: {error_msg}"


# ============================================================================
# RAGç³»ç»Ÿ
# ============================================================================
class RAGSystem:
    def __init__(self, documents: List[Dict]):
        self.retriever = BM25Retriever(documents)
        self.generator = FlexibleAPIGenerator()
    
    def answer_question(
        self, 
        query: str, 
        top_k: int = 10,
        model_name: Optional[str] = None
    ) -> Dict:
        """
        å›ç­”é—®é¢˜
        
        Args:
            query: é—®é¢˜
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            model_name: æ¨¡å‹æ˜¾ç¤ºåç§°ï¼ˆå¦‚æœè¦åˆ‡æ¢æ¨¡å‹ï¼‰
        """
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ï¼Œå…ˆåˆ‡æ¢
        if model_name:
            self.generator.switch_model(model_name)
        
        print(f"\n{'='*70}")
        print(f"â“ é—®é¢˜: {query}")
        print(f"ğŸ¤– æ¨¡å‹: {self.generator.current_model}")
        print(f"{'='*70}")
        
        # æ£€ç´¢
        print(f"ğŸ” æ£€ç´¢å‰ {top_k} ä¸ªç›¸å…³æ–‡æ¡£...")
        retrieved_docs = self.retriever.search(query, top_k=top_k)
        print(f"âœ… æ£€ç´¢å®Œæˆ")
        
        # ç”Ÿæˆ
        answer = self.generator.generate(query, retrieved_docs)
        
        return {
            'question': query,
            'answer': answer,
            'retrieved_docs': retrieved_docs,
            'model_used': self.generator.current_model
        }


# ============================================================================
# Gradioç•Œé¢ï¼ˆå¸¦æ¨¡å‹é€‰æ‹©å™¨ï¼‰
# ============================================================================
def create_interface(rag_system: RAGSystem):
    """åˆ›å»ºå¸¦æ¨¡å‹é€‰æ‹©åŠŸèƒ½çš„ç•Œé¢"""
    
    # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    available_models = rag_system.generator.get_available_model_names()
    
    if not available_models:
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼")
    
    def format_docs(docs: List[Dict]) -> str:
        result = "### ğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£\n\n"
        for i, doc in enumerate(docs, 1):
            result += f"**{i}. æ–‡æ¡£ {doc['id']}** (ç›¸å…³åº¦: {doc['score']:.4f})\n\n"
            result += f"{doc['text'][:200]}...\n\n---\n\n"
        return result
    
    def query_handler(question: str, top_k: int, model_choice: str) -> Tuple[str, str, str]:
        """å¤„ç†æŸ¥è¯¢å¹¶è¿”å›ç»“æœ"""
        if not question.strip():
            return "è¯·è¾“å…¥é—®é¢˜", "", "æœªä½¿ç”¨æ¨¡å‹"
        
        # æ‰§è¡ŒRAGï¼Œä¼ å…¥é€‰æ‹©çš„æ¨¡å‹
        result = rag_system.answer_question(question, top_k, model_choice)
        
        answer = result['answer']
        docs = format_docs(result['retrieved_docs'])
        model_info = f"**å½“å‰ä½¿ç”¨æ¨¡å‹**: `{result['model_used']}`"
        
        return answer, docs, model_info
    
    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="RAGç³»ç»Ÿ - å¯é€‰æ‹©æ¨¡å‹", theme=gr.themes.Soft()) as demo:
        
        gr.Markdown(
            """
            # ğŸ¤– RAGé—®ç­”ç³»ç»Ÿï¼ˆå¯é€‰æ‹©æ¨¡å‹ç‰ˆæœ¬ï¼‰
            
            åœ¨ä¸‹æ–¹é€‰æ‹©ä¸åŒçš„Qwenæ¨¡å‹ï¼Œä½“éªŒä¸åŒæ€§èƒ½å’Œé€Ÿåº¦çš„å¹³è¡¡
            """
        )
        
        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥åŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ è¾“å…¥åŒºåŸŸ")
                
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    placeholder="ä¾‹å¦‚: Where was Barack Obama born?",
                    lines=3
                )
                
                # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰èœå•ï¼ˆé‡ç‚¹ï¼ï¼‰
                model_selector = gr.Dropdown(
                    choices=available_models,
                    value=available_models[0],  # é»˜è®¤é€‰ç¬¬ä¸€ä¸ª
                    label="ğŸ¤– é€‰æ‹©Qwenæ¨¡å‹",
                    info="ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„æ€§èƒ½å’Œé€Ÿåº¦"
                )
                
                # æ£€ç´¢å‚æ•°
                top_k = gr.Slider(
                    minimum=1,
                    maximum=20,
                    value=10,
                    step=1,
                    label="ğŸ“š æ£€ç´¢æ–‡æ¡£æ•°é‡"
                )
                
                submit_btn = gr.Button("ğŸš€ æäº¤æŸ¥è¯¢", variant="primary", size="lg")
            
            # å³ä¾§ï¼šè¾“å‡ºåŒºåŸŸ
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¡ è¾“å‡ºåŒºåŸŸ")
                
                # æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹
                model_info_display = gr.Markdown(
                    value=f"**å½“å‰æ¨¡å‹**: `{rag_system.generator.current_model}`",
                )
                
                answer_output = gr.Textbox(
                    label="ğŸ“ ç”Ÿæˆçš„ç­”æ¡ˆ",
                    lines=6,
                    show_copy_button=True
                )
                
                docs_output = gr.Markdown(
                    label="ğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£"
                )
        
        # ç»‘å®šäº‹ä»¶
        submit_btn.click(
            fn=query_handler,
            inputs=[question_input, top_k, model_selector],
            outputs=[answer_output, docs_output, model_info_display]
        )
        
        # ç¤ºä¾‹é—®é¢˜
        gr.Examples(
            examples=[
                ["Where was Barack Obama born?"],
                ["What is the capital of France?"],
            ],
            inputs=question_input
        )
        
        # åº•éƒ¨è¯´æ˜
        gr.Markdown(
            """
            ---
            ### ğŸ“Š ç³»ç»Ÿä¿¡æ¯
            
            **å¯ç”¨æ¨¡å‹æ•°é‡**: {num_models}  
            **æ£€ç´¢æ–¹æ³•**: BM25  
            **æ–‡æ¡£åº“å¤§å°**: 144,718 ä¸ªæ–‡æ¡£
            """.format(num_models=len(available_models))
        )
    
    return demo


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
def main():
    print("\n" + "="*70)
    print("ğŸš€ RAGç³»ç»Ÿå¯åŠ¨ï¼ˆå¯é€‰æ‹©æ¨¡å‹ç‰ˆæœ¬ï¼‰")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    data_loader = DataLoader()
    documents = data_loader.get_documents()
    
    # åˆ›å»ºç³»ç»Ÿï¼ˆä¼šè‡ªåŠ¨æµ‹è¯•æ‰€æœ‰æ¨¡å‹ï¼‰
    rag_system = RAGSystem(documents)
    
    # æµ‹è¯•
    print("\n" + "="*70)
    print("ğŸ§ª æµ‹è¯•ç³»ç»Ÿ")
    print("="*70)
    
    test_result = rag_system.answer_question("Where was Barack Obama born?", top_k=10)
    print(f"\nğŸ’¡ ç­”æ¡ˆ: {test_result['answer']}")
    
    # å¯åŠ¨ç•Œé¢
    print("\n" + "="*70)
    print("ğŸŒ å¯åŠ¨Webç•Œé¢")
    print("="*70)
    print(f"å¯ç”¨æ¨¡å‹: {', '.join(rag_system.generator.get_available_model_names())}")
    
    demo = create_interface(rag_system)
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )


if __name__ == "__main__":
    main()