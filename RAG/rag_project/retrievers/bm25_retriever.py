# retrievers/bm25_retriever.py

"""
BM25æ£€ç´¢å™¨æ¨¡å— - retrievers/bm25_retriever.py

å®ç°åŸºäºBM25ç®—æ³•çš„æ–‡æ¡£æ£€ç´¢
"""

from typing import List, Dict
import bm25s
from config import BM25_CONFIG
# å¯¼å…¥æˆ‘ä»¬è‡ªå·±ç¼–å†™çš„ TextTokenizer
from TextTokenizer import TextTokenizer


class BM25Retriever:
    """BM25æ£€ç´¢å™¨"""
    
    def __init__(self, documents: List[Dict]):
        """
        åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å«idå’Œtext
        """
        print("\nğŸ” æ­£åœ¨åˆå§‹åŒ–BM25æ£€ç´¢å™¨...")
        
        self.documents = documents
        self.doc_ids = [doc['id'] for doc in documents]
        
        # 1. åˆå§‹åŒ–æˆ‘ä»¬è‡ªå®šä¹‰çš„åˆ†è¯å™¨
        # æ³¨æ„: æˆ‘ä»¬ä» BM25_CONFIG ä¸­è·å–è¯­è¨€ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
        self.tokenizer = TextTokenizer(language=BM25_CONFIG["stemmer_language"])
        
        # å‡†å¤‡æ–‡æ¡£æ–‡æœ¬
        corpus_texts = [doc['text'] for doc in documents]
        
        # 2. ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„åˆ†è¯å™¨å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯
        print("   æ­£åœ¨ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯...")
        corpus_tokens = self.tokenizer.process(corpus_texts)
        
        # 3. åˆ›å»ºBM25ç´¢å¼•
        print("   æ­£åœ¨åˆ›å»ºBM25ç´¢å¼•...")
        self.retriever = bm25s.BM25()
        self.retriever.index(corpus_tokens)
        
        print(f"âœ… BM25ç´¢å¼•åˆ›å»ºå®Œæˆï¼ç´¢å¼•äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    def search(self, query: str, top_k: int = 10) -> List[Dict]:
        """
        æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰Kä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
        
        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ï¼ŒåŒ…å«idã€textå’Œscore
        """
        # ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        query_tokens = self.tokenizer.process(query)
        
        # æ£€ç´¢
        results, scores = self.retriever.retrieve(query_tokens, k=top_k)
        
        # æ•´ç†ç»“æœ
        retrieved_docs = []
        for idx, score in zip(results[0], scores[0]):
            retrieved_docs.append({
                'id': self.doc_ids[idx],
                'text': self.documents[idx]['text'],
                'score': float(score)
            })
        
        return retrieved_docs
    
    def batch_search(self, queries: List[str], top_k: int = 10) -> List[List[Dict]]:
        """
        æ‰¹é‡æ£€ç´¢
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            top_k: æ¯ä¸ªæŸ¥è¯¢è¿”å›å‰Kä¸ªæ–‡æ¡£
        
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨çš„åˆ—è¡¨
        """
        # ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨è¿›è¡Œæ‰¹é‡åˆ†è¯
        query_tokens = self.tokenizer.process(queries)
        
        # æ‰¹é‡æ£€ç´¢
        results, scores = self.retriever.retrieve(query_tokens, k=top_k)
        
        # æ•´ç†æ‰€æœ‰æŸ¥è¯¢çš„ç»“æœ
        all_results = []
        for i in range(len(queries)):
            retrieved_docs = []
            for idx, score in zip(results[i], scores[i]):
                retrieved_docs.append({
                    'id': self.doc_ids[idx],
                    'text': self.documents[idx]['text'],
                    'score': float(score)
                })
            all_results.append(retrieved_docs)
            
        return all_results
    
    def get_name(self) -> str:
        """è¿”å›æ£€ç´¢å™¨åç§°"""
        return "BM25"