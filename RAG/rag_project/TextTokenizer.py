# text_tokenizer.py (ä¿®å¤ç‰ˆ)

import re
from typing import List, Union

import Stemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# --- NLTK æ•°æ®ä¸‹è½½æ£€æŸ¥ (å¯é€‰, ä½†å»ºè®®) ---
try:
    stopwords.words('english')
except LookupError:
    print("æ­£åœ¨ä¸‹è½½ NLTK 'stopwords' æ•°æ®...")
    import nltk
    nltk.download('stopwords', quiet=True)
try:
    word_tokenize("test")
except LookupError:
    print("æ­£åœ¨ä¸‹è½½ NLTK 'punkt' æ•°æ®...")
    import nltk
    nltk.download('punkt', quiet=True)
# -----------------------------------------


class TextTokenizer:
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„æ–‡æœ¬åˆ†è¯å™¨ï¼Œç”¨äºæ–‡æœ¬æ¸…æ´—ã€åˆ†è¯ã€åœç”¨è¯ç§»é™¤å’Œè¯å¹²æå–ã€‚
    """
    def __init__(self, language: str = "english"):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨ã€‚
        
        Args:
            language (str): ç”¨äºè¯å¹²æå–å’Œåœç”¨è¯çš„è¯­è¨€ã€‚
        """
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–è‡ªå®šä¹‰æ–‡æœ¬åˆ†è¯å™¨ (è¯­è¨€: {language})...")
        self.stemmer = Stemmer.Stemmer(language)
        # ä½¿ç”¨é›†åˆ(set)ä»¥è·å¾—æ›´å¿«çš„æŸ¥æ‰¾é€Ÿåº¦
        self.stopwords = set(stopwords.words(language))
        print("âœ… è‡ªå®šä¹‰æ–‡æœ¬åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆï¼")

    def _process_single_text(self, text: str) -> List[str]:
        """
        å¯¹å•ä¸ªå­—ç¬¦ä¸²æ‰§è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹ã€‚

        1. è½¬æ¢ä¸ºå°å†™ã€‚
        2. ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—ã€‚
        3. åˆ†è¯ã€‚
        4. ç§»é™¤åœç”¨è¯ã€‚
        5. è¯å¹²æå–ã€‚
        """
        # 1. è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # 2. ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œæ•°å­— (åªä¿ç•™å­—æ¯å’Œç©ºæ ¼)
        text = re.sub(r'[^a-z\s]', '', text)
        
        # 3. åˆ†è¯
        tokens = word_tokenize(text)
        
        # 4. ç§»é™¤åœç”¨è¯ï¼ˆåœ¨è¯å¹²æå–å‰ï¼‰
        tokens = [token for token in tokens if token not in self.stopwords and len(token) > 1]
        
        # 5. è¯å¹²æå– - ä½¿ç”¨ stemWords (æ‰¹é‡å¤„ç†)
        # âœ… ä¿®å¤ï¼šä½¿ç”¨ stemWords è€Œä¸æ˜¯ stem
        if tokens:  # åªæœ‰å½“tokenséç©ºæ—¶æ‰è°ƒç”¨
            processed_tokens = self.stemmer.stemWords(tokens)
        else:
            processed_tokens = []
        
        return processed_tokens

    def process(self, text_input: Union[str, List[str]]) -> List[List[str]]:
        """
        å¤„ç†å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå¹¶è¿”å› bm25s åº“æ‰€éœ€çš„æ ¼å¼ã€‚

        Args:
            text_input: å•ä¸ªæŸ¥è¯¢å­—ç¬¦ä¸²æˆ–æ–‡æ¡£å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

        Returns:
            ä¸€ä¸ªåŒ…å«å¤„ç†å token åˆ—è¡¨çš„åˆ—è¡¨ (List[List[str]])ã€‚
        """
        if isinstance(text_input, str):
            # å¦‚æœè¾“å…¥æ˜¯å•ä¸ªå­—ç¬¦ä¸², è¿”å›åŒ…å«ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨
            return [self._process_single_text(text_input)]
        
        if isinstance(text_input, list):
            # å¦‚æœè¾“å…¥æ˜¯åˆ—è¡¨, å¯¹å…¶ä¸­æ¯ä¸ªå­—ç¬¦ä¸²è¿›è¡Œå¤„ç†
            return [self._process_single_text(doc) for doc in text_input]
        
        raise TypeError("è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸² (str) æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ (List[str])")